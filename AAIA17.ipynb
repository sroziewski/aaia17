{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/opt/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, IsolationForest, RandomForestClassifier, GradientBoostingClassifier)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPrefix = '/home/data/aaia17/'\n",
    "def writeToFile(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    f.close()\n",
    "def printify(t):\n",
    "    for x in t:\n",
    "        print(x)\n",
    "def readDataAsFrame(fNames):\n",
    "    frame = []\n",
    "    for fileName in fNames:\n",
    "        fData = dataPrefix+fileName\n",
    "        frame.append(pandas.read_csv(fData))\n",
    "    return pandas.concat(frame)        \n",
    "def getNewForIndexes(dataFrame, names):\n",
    "    X_selectedFeature = np.empty((len(dataFrame), 0), float)\n",
    "    for name in names:\n",
    "        X_selectedFeature = np.column_stack([X_selectedFeature, dataFrame[name].values])\n",
    "    return X_selectedFeature\n",
    "def dataScaling(data, scaler):\n",
    "    data = data/(data+1)\n",
    "    return scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading from all chunks and making one  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingFiles = ['trainingData_tabular_chunk1.csv', 'trainingData_tabular_chunk2.csv',\n",
    "                 'trainingData_tabular_chunk3.csv', 'trainingData_tabular_chunk4.csv']\n",
    "rawTrainingDataset = readDataAsFrame(trainingFiles)\n",
    "\n",
    "testFiles = ['testData_tabular.csv']\n",
    "rawTestDataset = readDataAsFrame(testFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(rawTrainingDataset.groupby('decision').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand chosen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actually, we don't use it !\n",
    "\n",
    "names = ['opponent.hp', 'opponent.played_minions_count', 'player.armor','player.hp', 'player.hand_count', \n",
    "         'player.played_minions_count', 'opponent.played.nOfCards', 'opponent.played.attack', 'opponent.played.crystals_cost'\n",
    "         ,'opponent.played.hp_current', 'opponent.played.hp_max', 'player.played.nOfCards', 'player.played.attack',\n",
    "        'player.played.crystals_cost', 'player.played.hp_current', 'player.played.hp_max', 'player.hand.nOfCards']\n",
    "X_train_handChosen = getNewForIndexes(rawTrainingDataset, names)\n",
    "X_test_handChosen  = getNewForIndexes(rawTestDataset, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Outliers detection and removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Actually, we don't use it !\n",
    "\n",
    "YRawTrainingDataset = rawTrainingDataset.values[:,1]\n",
    "XRawTrainingDataset = rawTrainingDataset.values[:,2:45]\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "clf = IsolationForest(max_samples=10000, random_state=rng, n_jobs=-1)\n",
    "clf.fit(XRawTrainingDataset)\n",
    "\n",
    "trainingOutliers = clf.predict(XRawTrainingDataset)\n",
    "\n",
    "XTrainigDatasetWithoutOutliers = XRawTrainingDataset[trainingOutliers>0]\n",
    "YTrainingDatasetWithoutOutliers = YRawTrainingDataset[trainingOutliers>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Actually, we don't use it !\n",
    "\n",
    "estimator = LogisticRegression(random_state=rng)\n",
    "selector = RFECV(estimator, step=1, cv=10, scoring='roc_auc', n_jobs=-1)\n",
    "XTrainingSelected = selector.fit_transform(Xtry, Ytry)\n",
    "pickle.dump(XTrainingSelected, open(dataPrefix+'XTrainingSelected.store',\"wb\"))\n",
    "\n",
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "XTrainingDatasetScaledWithOutliers = dataScaling(XRawTrainingDataset, min_max_scaler)\n",
    "XTestDatasetScaled = dataScaling(XRawTestDataset, min_max_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating train and validation probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_size = 0.30\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(XTrainingDatasetScaledWithOutliers, \n",
    "        YRawTrainingDataset, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actually, we don't use it !\n",
    "%%time\n",
    "\n",
    "seed = 7\n",
    "scoring = 'roc_auc'\n",
    "models = []\n",
    "models.append(('RFC', RandomForestClassifier(n_jobs=-1, n_estimators=40)))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Main Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = GradientBoostingClassifier(n_estimators=1000, max_depth=4)\n",
    "clf = clf.fit(XTrainingDatasetScaledWithOutliers, YRawTrainingDataset)\n",
    "predictedTestWon = clf.predict_proba(XTestDatasetScaled)[:,1]\n",
    "writeToFile(predictedTestWon, dataPrefix+'predicted22.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
